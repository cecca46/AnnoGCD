{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from sklearn.decomposition import PCA\n",
    "import math\n",
    "from scipy.spatial import cKDTree\n",
    "from numbers import Number\n",
    "import torch.nn.functional as F\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "from sklearn import mixture\n",
    "import torch.optim as optim\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import scanpy as sc\n",
    "from anndata import AnnData\n",
    "import torch\n",
    "import math\n",
    "from collections import Counter\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from scipy.stats import mode\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "import umap\n",
    "from torch_geometric.nn import GCNConv\n",
    "import matplotlib\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = {\n",
    "    \"Skin-SHARE\": {\n",
    "    'Dermal Fibroblast': (0.5, 0.5, 0.5),          # Gray for fibroblast cells\n",
    "    'Dermal Papilla': (0.52, 0.52, 0.52),         # Slightly different gray\n",
    "    'TAC-1': (0.9, 0.8, 0.1),                     # Yellow for basal keratinocyte progenitors\n",
    "    'Mix': (0.6, 0.6, 0.6),                       # Light gray for mixed cells\n",
    "    'IRS': (0.6, 0.1, 0.5),                       # Purple for inner root sheath\n",
    "    'Basal': (0.9, 0.82, 0.12),                   # Slightly different yellow\n",
    "    'K6+ Bulge Companion Layer': (0.58, 0.1, 0.48),  # Slightly different purple\n",
    "    'Medulla': (0.62, 0.12, 0.52),                # Another shade of purple\n",
    "    'alowCD34+ bulge': (0.64, 0.14, 0.54),        # Another shade of purple\n",
    "    'Isthmus': (0.66, 0.16, 0.56),                # Another shade of purple\n",
    "    'ORS': (0.68, 0.18, 0.58),                    # Another shade of purple\n",
    "    'Infundibulum': (0.7, 0.2, 0.6),              # Another shade of purple\n",
    "    'Spinous': (0.92, 0.84, 0.14),                # Another shade of yellow\n",
    "    'ahighCD34+ bulge': (0.72, 0.22, 0.62),       # Another shade of purple\n",
    "    'TAC-2': (0.94, 0.86, 0.16),                  # Another shade of yellow\n",
    "    'Macrophage DC': (0.2, 0.6, 0.3),             # Green for immune cells\n",
    "    'Endothelial': (0.3, 0.7, 0.4),               # Slightly different green\n",
    "    'Dermal Sheath': (0.54, 0.54, 0.54),          # Another gray shade\n",
    "    'Sebaceous Gland': (0.62, 0.62, 0.62),        # Another light gray\n",
    "    'Granular': (0.96, 0.88, 0.18),               # Another shade of yellow\n",
    "    'Hair Shaft-cuticle.cortex': (0.74, 0.24, 0.64), # Another shade of purple\n",
    "    'Schwann Cell': (0.64, 0.64, 0.64),           # Another light gray\n",
    "    'Melanocyte': (0.66, 0.66, 0.66)              # Another light gray\n",
    "},\n",
    "    \"BM-CITE\": {\n",
    "    'CD14 Mono': (0.1, 0.4, 0.6),        # Dark blue\n",
    "    'CD16 Mono': (0.1, 0.3, 0.5),        # Lighter blue\n",
    "    'CD4 Memory': (0.9, 0.4, 0.0),       # Dark orange\n",
    "    'CD4 Naive': (1.0, 0.6, 0.2),        # Light orange\n",
    "    'CD8 Effector': (0, 1, 0),     # Dark green\n",
    "    'CD8 Memory': (0.2, 0.7, 0.2),       # Medium green\n",
    "    'CD8 Naive': (0.7, 1.0, 0.7),        # Light green\n",
    "    'GMP': (0.5, 0.3, 0.7),              # Purple\n",
    "    'HSC': (0.0, 0.5, 0.7),              # Cyan\n",
    "    'LMPP': (0.0, 0.4, 0.6),             # Darker Cyan\n",
    "    'MAIT': (0.6, 0.2, 0.2),             # Maroon\n",
    "    'Memory B': (0.9, 0.3, 0.3),      # Dark red\n",
    "    'NK': (0.4, 0.3, 0.3),           # Pink\n",
    "    'Naive B': (1, 0.0, 0.0),          # Light purple\n",
    "    'Plasmablast': (0.6, 0.6, 0.1),      # Olive\n",
    "    'Prog_B': (0.5, 0.3, 0.1),             # Brown\n",
    "    'Prog_DC': (0.6, 0.4, 0.2),          # Darker brown\n",
    "    'Prog_Mk': (0.7, 0.5, 0.3),          # Even darker brown\n",
    "    'Prog_RBC': (0.8, 0.6, 0.4),         # Light brown\n",
    "    'Treg': (0.9, 0.5, 0.3),             # Coral\n",
    "    'cDC2': (0.3, 0.3, 0.9),             # Indigo\n",
    "    'gdT': (0.2, 0.5, 0.3),              # Sea green\n",
    "    'pDC': (0.1, 0.6, 0.6)},\n",
    "    \n",
    "    \"PBMC-TEA\": {\n",
    "    'T.CD4.Memory': (0.9, 0.4, 0.0),          # Dark orange for CD4 Memory T cells\n",
    "    'Mono.CD14': (0.1, 0.4, 0.6),             # Dark blue for CD14+ Monocytes\n",
    "    'B.Naive': (1.0, 0.6, 0.2),               # Light orange for Naive B cells\n",
    "    'T.CD8.Effector': (0, 1, 0),              # Bright green for CD8 Effector T cells\n",
    "    'T.CD4.Naive': (1.0, 0.65, 0.25),         # Slightly lighter orange for CD4 Naive T cells\n",
    "    'B.Activated': (0.95, 0.55, 0.15),        # Darker orange for Activated B cells\n",
    "    'T.CD8.Naive': (0.7, 1.0, 0.7),           # Light green for CD8 Naive T cells\n",
    "    'DC.Myeloid': (0.3, 0.3, 0.9),            # Indigo for Myeloid Dendritic Cells\n",
    "    'NK': (0.4, 0.3, 0.3),                    # Pink for Natural Killer cells\n",
    "    'Platelets': (0.6, 0.4, 0.4),             # Dusty pink for Platelets\n",
    "    'Mono.CD16': (0.1, 0.35, 0.55),           # Lighter blue for CD16+ Monocytes\n",
    "    'T.DoubleNegative': (0.5, 0.5, 0.1)},\n",
    "    \n",
    "    \"PBMC-Multiome\": {\n",
    "    'CD4 Naive': (0.9, 0.8, 0.4),         # Lighter orange, indicative of naive state\n",
    "    'CD4 Memory': (0.9, 0.6, 0.2),        # Darker orange, indicative of memory state\n",
    "    'CD8 Naive': (0.7, 0.9, 0.4),         # Lighter green, for naive CD8\n",
    "    'CD8 effector': (0, 0.8, 0),          # Dark green, for effector CD8\n",
    "    'CD16+ Monocytes': (0.1, 0.5, 0.6),   # Deep teal, distinguishing them from CD14+ Monocytes\n",
    "    'CD14+ Monocytes': (0.1, 0.4, 0.6),   # Dark blue, common for monocytes\n",
    "    'B cell progenitor': (1, 0.5, 0.75),  # Light pink, indicating an early stage in B cell development\n",
    "    'pre-B cell': (1, 0.4, 0.7),          # Slightly darker pink, a later stage of B cell progenitor\n",
    "    'Dendritic cell': (0.5, 0.5, 0.9),    # Bright purple, unique among APCs\n",
    "    'Double negative T cell': (0.6, 0.3, 0.5),  # Purplish-pink, unique due to their DN nature\n",
    "    'pDC': (0.4, 0.6, 0.9),               # Lighter purple, indicating a specialized dendritic cell type\n",
    "    'NK cell': (0.9, 0.3, 0.3),           # Red, vibrant and distinct for natural killer cells\n",
    "    'Platelets': (0.8, 0.6, 0.4)          # Muted brown, indicative of their different nature\n",
    "},\n",
    "    \n",
    "    \"PBMC-DOGMA\": {\n",
    "    'CD4 Naive': (0.9, 0.8, 0.4),\n",
    "    'CD4 TCM': (0.9, 0.75, 0.3),          # Central memory, close to naive but slightly darker\n",
    "    'gdT': (0.4, 0.5, 0.2),                # Unique olive green, distinct from other T cells\n",
    "    'CD8 TEM': (0.2, 0.7, 0.3),            # Effector memory, darker green\n",
    "    'B naive': (0.6, 0.8, 0.9),            # Light blue for naive B cells\n",
    "    'CD8 Naive': (0.6, 0.9, 0.5),          # Lighter green for naive CD8\n",
    "    'Treg': (0.9, 0.5, 0.6),               # Soft pink for regulatory T cells\n",
    "    'CD14 Mono': (0.1, 0.4, 0.6),          # Standard dark blue for monocytes\n",
    "    'Eryth': (0.8, 0.3, 0.3),              # Reddish for erythrocytes (even if not immune cells, assuming hematopoietic context)\n",
    "    'NK': (0.7, 0.2, 0.2),                 # Dark red for natural killer cells\n",
    "    'B intermediate': (0.55, 0.75, 0.95),  # Intermediate blue for intermediate B cells\n",
    "    'MAIT': (0.5, 0.4, 0.2),               # Muddy gold for mucosal-associated invariant T cells\n",
    "    'ASDC': (0.5, 0.5, 0.8),               # Light purple for activated stromal dendritic cells\n",
    "    'dnT': (0.45, 0.6, 0.25),              # Dark olive for double negative T\n",
    "    'CD4 TEM': (0.85, 0.7, 0.35),          # Effector memory, near CD4 Naive but slightly darker\n",
    "    'CD8 TCM': (0.5, 0.85, 0.4),           # Central memory, close to CD8 Naive but distinct\n",
    "    'CD16 Mono': (0.15, 0.45, 0.65),       # Slightly lighter blue than CD14+ Monocytes\n",
    "    'ILC': (0.3, 0.6, 0.3),                # Intermediate green for innate lymphoid cells\n",
    "    'B memory': (0.5, 0.7, 0.9),           # Deeper blue for memory B cells\n",
    "    'cDC2': (0.4, 0.4, 0.9),               # Indigo for conventional dendritic cell type 2\n",
    "    'pDC': (0.35, 0.35, 0.85),             # Lighter indigo for plasmacytoid dendritic cells\n",
    "    'NK_CD56bright': (0.75, 0.25, 0.25),   # Bright red for CD56 bright NK cells\n",
    "    'CD4 CTL': (0.88, 0.65, 0.4),          # Cytotoxic T lymphocytes, close to CD4 colors but distinct\n",
    "    'HSPC': (0.9, 0.4, 0.4),               # Hematopoietic stem and progenitor cells, distinct red\n",
    "    'Platelet': (0.8, 0.6, 0.4),           # Muted brown for platelets\n",
    "    'Plasmablast': (0.6, 0.6, 0.1),        # Olive for plasmablasts\n",
    "    'CD4 Proliferating': (0.93, 0.83, 0.45), # Very light orange, indicating active proliferation\n",
    "    'CD4': (0.9, 0.8, 0.4),\n",
    "    'CD8': (0.6, 0.9, 0.5),\n",
    "    },\n",
    "    \n",
    "    \"LUNG-CITE\": {\n",
    "    'T.CD.EM': (0.2, 0.7, 0.3),           # Dark green for T CD Effector Memory\n",
    "    'mDC.Lung': (0.5, 0.5, 0.9),          # Light purple for myeloid DC in lung\n",
    "    'NK': (0.7, 0.2, 0.2),                # Dark red for natural killer cells\n",
    "    'Mono': (0.1, 0.4, 0.6),              # Dark blue for monocytes\n",
    "    'T.CD4': (0.9, 0.6, 0.2),             # Orange for CD4 T cells\n",
    "    'B.Plasma': (0.8, 0.5, 0.9),          # Light violet for plasma B cells\n",
    "    'T.CD8': (0.6, 0.8, 0.2),             # Olive green for CD8 T cells\n",
    "    'T.CD8.CM': (0.55, 0.75, 0.25),       # Slightly different olive for CD8 Central Memory\n",
    "    'B.Blood': (0.6, 0.6, 0.9),           # Soft blue for blood B cells\n",
    "    'mDC.Blood': (0.45, 0.45, 0.85),      # Slightly different light purple for myeloid DC in blood\n",
    "    'Macro': (0.9, 0.3, 0.3),             # Coral for macrophages\n",
    "    'Fibro': (0.6, 0.6, 0.6),             # Grey for fibroblasts\n",
    "    'T.CD8.EM': (0.3, 0.7, 0.4),          # Different shade of green for CD8 Effector Memory\n",
    "    'T.Treg': (0.9, 0.5, 0.6),            # Soft pink for T regulatory cells\n",
    "    'B.Lung': (0.65, 0.65, 0.95),         # Different soft blue for lung B cells\n",
    "    'Mono.Int': (0.15, 0.45, 0.65),       # Slightly different blue for intermediate monocytes\n",
    "    'T.Prolif': (0.7, 0.85, 0.4),         # Bright olive for proliferating T cells\n",
    "    'pDC': (0.35, 0.35, 0.9),             # Light indigo for plasmacytoid dendritic cells\n",
    "    'Epithelia': (0.8, 0.4, 0.4)          # Muted red for epithelial cells\n",
    "}\n",
    "}\n",
    "\n",
    "additional = {\n",
    "    -1: (0.34, 0.92, 0.29),\n",
    "    0: (0.74, 0.54, 0.41),\n",
    "    1: (0.13, 0.57, 0.76),\n",
    "    2: (0.88, 0.35, 0.47),\n",
    "    3: (0.69, 0.16, 0.82),\n",
    "    4: (0.46, 0.87, 0.34),\n",
    "    5: (0.25, 0.56, 0.96),\n",
    "    6: (0.84, 0.38, 0.94),\n",
    "    7: (0.78, 0.61, 0.52),\n",
    "    8: (0.17, 0.93, 0.63),\n",
    "    9: (0.33, 0.48, 0.75),\n",
    "    10: (0.54, 0.84, 0.12),\n",
    "    11: (0.69, 0.18, 0.87),\n",
    "    12: (0.37, 0.91, 0.24),\n",
    "    13: (0.63, 0.26, 0.56),\n",
    "    14: (0.71, 0.82, 0.48),\n",
    "    15: (0.29, 0.72, 0.59),\n",
    "    16: (0.85, 0.36, 0.97),\n",
    "    17: (0.19, 0.79, 0.88),\n",
    "    18: (0.61, 0.47, 0.35),\n",
    "    19: (0.76, 0.23, 0.67),\n",
    "    20: (0.91, 0.54, 0.11),\n",
    "    21: (0.42, 0.78, 0.82),\n",
    "    22: (0.56, 0.69, 0.37)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print ('Using ', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BM-CITE  LUNG-CITE  PBMC-DOGMA  PBMC-Multiome  PBMC-TEA \n",
    "dataset = 'BM-CITE'\n",
    "data_dir = '/Users/francescoceccarelli/Library/Mobile Documents/com~apple~CloudDocs/PhD/MultiOmics/preprocessed_data'\n",
    "files = [f for f in listdir(join(data_dir, dataset)) if isfile(join(data_dir, dataset, f))]\n",
    "\n",
    "color_map = cmap[dataset]\n",
    "for k, v in additional.items():\n",
    "    color_map[k] = v\n",
    "\n",
    "for file in files:\n",
    "    if ('EXPR' in file):\n",
    "        exp = pd.read_csv(join(data_dir, dataset, file))\n",
    "        exp = exp.T\n",
    "        exp.columns = exp.iloc[0]\n",
    "        exp = exp.drop(exp.index[0])\n",
    "\n",
    "    if ('META' in file): \n",
    "        meta = pd.read_csv(join(data_dir, dataset, file))\n",
    "        meta.index = meta.iloc[:,0]\n",
    "        meta = meta.drop(meta.columns[0], axis=1)\n",
    "\n",
    "assert (np.all(meta.index == exp.index))\n",
    "print ('Expression: ', exp.shape)\n",
    "print ('Meta: ', meta.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(meta['celltype'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out low count cells\n",
    "# Change labels to be more consistent\n",
    "min_count = 100\n",
    "exp['celltype'] = meta['celltype']\n",
    "\n",
    "if (dataset == 'BM-CITE'):\n",
    "     exp[\"celltype\"] = exp[\"celltype\"].replace(['CD8 Effector_1', 'CD8 Effector_2'], 'CD8 Effector')\n",
    "     exp[\"celltype\"] = exp[\"celltype\"].replace(['CD8 Memory_1', 'CD8 Memory_2'], 'CD8 Memory')\n",
    "     exp[\"celltype\"] = exp[\"celltype\"].replace(['Prog_B 1', 'Prog_B 2'], 'Prog_B')\n",
    "     exp[\"celltype\"] = exp[\"celltype\"].replace(['CD56 bright NK'], 'NK')\n",
    "     \n",
    "if (dataset == 'LUNG-CITE'):\n",
    "     exp[\"celltype\"] = exp[\"celltype\"].replace(['B.Plasma.1', 'B.Plasma.2'], 'B.Plasma')\n",
    "     exp[\"celltype\"] = exp[\"celltype\"].replace(['Mono.1', 'Mono.2', 'Mono.3'], 'Mono')\n",
    "\n",
    "if (dataset == 'PBMC-DOGMA'):\n",
    "      exp[\"celltype\"] = exp[\"celltype\"].replace(['CD4 Naive', 'CD4 TCM', 'CD4 TEM', ], 'CD4')\n",
    "      exp[\"celltype\"] = exp[\"celltype\"].replace(['CD8 Naive', 'CD8 TEM', 'CD8 TCM'], 'CD8')\n",
    "\n",
    "value_counts = exp['celltype'].value_counts()\n",
    "values_to_remove = value_counts[value_counts < min_count].index\n",
    "exp = exp[~exp['celltype'].isin(values_to_remove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(exp[\"celltype\"], return_counts=True)\n",
    "celltype_counts = exp['celltype'].value_counts()\n",
    "\n",
    "#Look at the distribution of celltypes\n",
    "print ('Total number of cell types: ', len(celltype_counts))\n",
    "print (celltype_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_index(class_counts):\n",
    "    \"\"\"\n",
    "    Calculate the Gini index using numpy for a list of class counts.\n",
    "\n",
    "    Args:\n",
    "    class_counts (list or numpy array): A list or numpy array where each element represents the number of instances in a class.\n",
    "\n",
    "    Returns:\n",
    "    float: The Gini index, where 0 represents perfect equality (all classes have the same number of instances),\n",
    "           and 1 represents maximal inequality (all instances are in one class).\n",
    "    \"\"\"\n",
    "    # Convert class_counts to a numpy array for vectorized operations if it's not already one\n",
    "    class_counts = np.array(class_counts)\n",
    "    # Total number of instances\n",
    "    total_instances = np.sum(class_counts)\n",
    "    if total_instances == 0:\n",
    "        return 0  # To handle the case where the dataset is empty and avoid division by zero\n",
    "\n",
    "    # Calculate the probability of each class (p_i)\n",
    "    class_probabilities = class_counts / total_instances\n",
    "    # Calculate the Gini index using the formula: G = 1 - sum(p_i^2)\n",
    "    gini = 1 - np.sum(class_probabilities ** 2)\n",
    "    return gini\n",
    "\n",
    "class_counts = celltype_counts.to_numpy()\n",
    "print(f\"Gini Index: {gini_index(class_counts):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into known and unknown classes\n",
    "spliting_method = 'most_common'\n",
    "if (spliting_method == 'most_common'):\n",
    "\n",
    "    known_celltypes = celltype_counts.nlargest(math.ceil(len(unique) / 2)).index\n",
    "    unknown_celltypes = celltype_counts.nsmallest(math.floor(len(unique) / 2)).index\n",
    "\n",
    "    known_genes = exp[exp['celltype'].isin(known_celltypes)]\n",
    "    unknown_genes = exp[exp['celltype'].isin(unknown_celltypes)]\n",
    "\n",
    "if (spliting_method == 'order'):\n",
    "    split_point = len(unique) // 2 if len(unique) % 2 == 0 else (len(unique) // 2) + 1\n",
    "\n",
    "    known_celltypes = unique[:split_point]\n",
    "    unknown_celltypes = unique[split_point:]\n",
    "\n",
    "    known_genes = exp[exp['celltype'].isin(known_celltypes)]\n",
    "    unknown_genes = exp[exp['celltype'].isin(unknown_celltypes)]\n",
    "\n",
    "if (spliting_method == 'random'):\n",
    "    known_celltypes = np.random.choice(unique, math.ceil(len(unique) / 2), replace=False)\n",
    "    unknown_celltypes = np.setdiff1d(unique, known_celltypes)\n",
    "\n",
    "    known_genes = exp[exp['celltype'].isin(known_celltypes)]\n",
    "    unknown_genes = exp[exp['celltype'].isin(unknown_celltypes)]\n",
    "    \n",
    "print ('Known classes: %d' % len(known_celltypes))\n",
    "print ('Unkown classes: %d' % len(unknown_celltypes))\n",
    "\n",
    "print (known_genes.shape)\n",
    "print (unknown_genes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "move_percentage = 0.3\n",
    "\n",
    "#Sample the known cells and move them to the unknown set\n",
    "group_sizes = known_genes.groupby('celltype').size()\n",
    "total_size = group_sizes.sum()\n",
    "weights = group_sizes / total_size\n",
    "row_weights = known_genes['celltype'].map(weights)\n",
    "\n",
    "# Sample rows with the computed weights\n",
    "num_samples = int(len(known_genes) * move_percentage) \n",
    "sampled_df = known_genes.sample(n=num_samples, weights=row_weights, replace=False)\n",
    "unknown_genes = pd.concat([unknown_genes, sampled_df])\n",
    "known_genes = known_genes.drop(sampled_df.index)\n",
    "sampled_indices = sampled_df.index\n",
    "\n",
    "print ('Known cells', known_genes.shape)\n",
    "print ('Unkown cells', unknown_genes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "celltype_counts = known_genes['celltype'].value_counts()\n",
    "print ('Distribution of known cells')\n",
    "print (celltype_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the cell types to integers\n",
    "# Assign integer values to known cell types in [0,..,K-1]\n",
    "# Assign integer values to unknown cell types in [K,..,N-1]\n",
    "\n",
    "known_types = known_genes['celltype'].unique()\n",
    "known_mapping = {cell_type: idx for idx, cell_type in enumerate(known_types)}\n",
    "known_genes['celltype'] = known_genes['celltype'].map(known_mapping)\n",
    "\n",
    "all_mapping = known_mapping.copy()\n",
    "next_int = len(known_mapping)\n",
    "\n",
    "# Assign integer values to unknown cell types\n",
    "for cell_type in unknown_genes['celltype'].unique():\n",
    "    if cell_type not in all_mapping:\n",
    "        all_mapping[cell_type] = next_int\n",
    "        next_int += 1\n",
    "\n",
    "# Apply the mapping to all_df\n",
    "unknown_genes['celltype'] = unknown_genes['celltype'].map(all_mapping)\n",
    "\n",
    "known_celltypes_names = known_celltypes\n",
    "unknown_celltypes_names = unknown_celltypes\n",
    "\n",
    "known_celltypes = pd.Series(known_celltypes).map(known_mapping)\n",
    "unknown_celltypes = pd.Series(unknown_celltypes).map(all_mapping)\n",
    "\n",
    "inv_known_mapping = {v: k for k, v in known_mapping.items()}\n",
    "inv_all_mapping = {v: k for k, v in all_mapping.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep track of the unknown cells for differential analysis\n",
    "unknown_cell_names = np.array(unknown_genes.index)\n",
    "\n",
    "celltypes_k = known_genes['celltype']\n",
    "known_genes = known_genes.drop('celltype', axis=1)\n",
    "known_genes_size = known_genes.shape[0]\n",
    "\n",
    "celltypes_u = unknown_genes['celltype']\n",
    "unknown_genes = unknown_genes.drop('celltype', axis=1)\n",
    "unknown_genes_size = unknown_genes.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA \n",
    "#Reduce dimension of the expression matrix\n",
    "reduction = 'PCA'\n",
    "pcs_n = 64\n",
    "if (reduction == 'PCA'):\n",
    "    print ('Computing PCA with %d components...' %pcs_n)\n",
    "    pca = PCA(n_components = pcs_n)\n",
    "    #known_genes = pd.DataFrame(pca.fit_transform(known_genes))\n",
    "    #unknown_genes = pd.DataFrame(pca.transform(unknown_genes))\n",
    "    all_genes = pd.concat([known_genes, unknown_genes])\n",
    "    all_genes = pd.DataFrame(pca.fit_transform(all_genes))\n",
    "    known_genes = all_genes.iloc[:known_genes_size]\n",
    "    unknown_genes = all_genes.iloc[known_genes_size:]\n",
    "\n",
    "known_genes['celltype'] = celltypes_k.tolist()\n",
    "unknown_genes['celltype'] = celltypes_u.tolist()\n",
    "\n",
    "print ('Known cells: ', known_genes.shape)\n",
    "print ('Unknown cells: ', unknown_genes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define all model inputs \n",
    "known_genes = known_genes.to_numpy(dtype=np.float16)\n",
    "unknown_genes = unknown_genes.to_numpy(dtype=np.float16)\n",
    "\n",
    "x_labeled = torch.from_numpy(known_genes[:,:-1]).float()\n",
    "y_labeled = torch.from_numpy(known_genes[:,-1])\n",
    "y_labeled = y_labeled.type(torch.LongTensor)\n",
    "\n",
    "x_unlabeled = torch.from_numpy(unknown_genes[:,:-1]).float()\n",
    "y_unlabeled = torch.from_numpy(unknown_genes[:,-1])\n",
    "y_unlabeled = y_unlabeled.type(torch.LongTensor)\n",
    "\n",
    "print (x_labeled.shape)\n",
    "print (y_labeled.shape)\n",
    "\n",
    "print (x_unlabeled.shape)\n",
    "print (y_unlabeled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedAutoEncoder(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=[32, 32], dropout=0.2, mask_prob=0.2):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self. mask_prob = torch.tensor(mask_prob)\n",
    "\n",
    "        self.encoder, self.mask_predictor, self.decoder = self.contrsuct_model(input_size, hidden_size, dropout)\n",
    "        \n",
    "    def contrsuct_model(self, input_size, hidden_size, dropout):\n",
    "        \n",
    "        # Dynamically create the encoder layers\n",
    "        layers = [nn.Dropout(p=dropout)]\n",
    "        input_size = input_size\n",
    "                \n",
    "        if isinstance(hidden_size, Number):\n",
    "            hidden_size = [hidden_size]\n",
    "            \n",
    "        hidden_size.insert(0, input_size)\n",
    "        \n",
    "        for idx in range(len(hidden_size)-2):\n",
    "            layers.extend([\n",
    "                nn.Linear(hidden_size[idx], hidden_size[idx+1]),\n",
    "                nn.LayerNorm(hidden_size[idx+1]),\n",
    "                nn.Mish(inplace=True)\n",
    "            ])\n",
    "        \n",
    "        # Last layer of the encoder without activation\n",
    "        layers.append(nn.Linear(hidden_size[-2], hidden_size[-1]))\n",
    "\n",
    "        encoder = nn.Sequential(*layers)\n",
    "\n",
    "        mask_predictor = nn.Linear(hidden_size[-1], input_size)\n",
    "        decoder = nn.Linear(in_features=hidden_size[-1] + input_size, out_features=input_size)\n",
    "        \n",
    "        return encoder, mask_predictor, decoder\n",
    "\n",
    "    def forward(self, X):\n",
    "        \n",
    "        if self.training:\n",
    "            corrupted_X, true_mask = self.mask_input(X)\n",
    "        else:\n",
    "            corrupted_X = X\n",
    "            true_mask = torch.ones_like(X)\n",
    "        \n",
    "        encoded = self.encoder(corrupted_X)\n",
    "        predicted_mask = self.mask_predictor(encoded)\n",
    "        masked_encoded = torch.cat([encoded, true_mask * corrupted_X], dim=-1)\n",
    "        reconstruction = self.decoder(masked_encoded)\n",
    "        \n",
    "        return [encoded, reconstruction]\n",
    "    \n",
    "    \n",
    "    def mask_input(self, X):\n",
    "        \n",
    "        should_swap = torch.bernoulli(self.mask_prob.to( X.device) * torch.ones((X.shape)).to(X.device))\n",
    "        corrupted_X = torch.where(should_swap == 1, X[torch.randperm(X.shape[0])], X)\n",
    "        masked = (corrupted_X != X).float()\n",
    "        return corrupted_X, masked\n",
    "\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, input_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "    \n",
    "class Predictor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(Predictor, self).__init__()\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, num_classes),\n",
    "            nn.Softmax(dim=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.predictor(x)\n",
    "    \n",
    "class MultiObjectiveLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiObjectiveLoss, self).__init__()\n",
    "        self.weights = nn.Parameter(torch.rand(2)) \n",
    "    \n",
    "    def forward(self, loss1, loss2):\n",
    "        # Compute the weighted sum of the loss terms\n",
    "        weights = F.softmax(self.weights, dim=0)\n",
    "        weighted_loss = self.weights[0] * loss1 + self.weights[1] * loss2\n",
    "        return weighted_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_labeled = x_labeled.to(device)\n",
    "y_labeled = y_labeled.to(device)\n",
    "x_unlabeled = x_unlabeled.to(device)\n",
    "y_unlabeled = y_unlabeled.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = x_labeled.shape[1]\n",
    "hidden_size = 32\n",
    "masked = False\n",
    "\n",
    "#all data for unsupervised autoencoder loss\n",
    "input_data = torch.cat((x_labeled, x_unlabeled), dim=0).to(device)\n",
    "\n",
    "# Instantiate the autoencoder, predictor & pseudo_predictor\n",
    "if masked:\n",
    "    autoencoder = MaskedAutoEncoder(input_size).float().to(device)\n",
    "    print (\"Using Masked Autoencoder...\")\n",
    "else:\n",
    "    autoencoder = Autoencoder(input_size, hidden_size).float().to(device)\n",
    "\n",
    "predictor = Predictor(input_size, hidden_size, len(known_celltypes)).float().to(device)\n",
    "\n",
    "#losses \n",
    "criterion_mse = nn.MSELoss() #autoencoder\n",
    "criterion_CE = nn.CrossEntropyLoss() #predictor\n",
    "\n",
    "# Initialize learnable weights for the three loss components\n",
    "# Start with equal weights\n",
    "loss_weights = nn.Parameter(torch.ones(2, device=device) / 2)  \n",
    "\n",
    "#optimizers \n",
    "autoencoder_optimizer = optim.AdamW(autoencoder.parameters(), lr=0.001)\n",
    "predictor_optimizer = optim.AdamW(predictor.parameters(), lr=0.001)\n",
    "optimizer_weights = optim.AdamW([loss_weights], lr=0.001)\n",
    "\n",
    "num_epochs = 500\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "   \n",
    "    # Autoencoder (unsupervised loss)\n",
    "    encoded, reconstructed = autoencoder(input_data)\n",
    "    autoencoder_loss = criterion_mse(reconstructed, input_data)\n",
    "    \n",
    "    #Predictor labeled (supervised loss)\n",
    "    predictions = predictor(x_labeled)\n",
    "    predictor_loss = criterion_CE(predictions, y_labeled)\n",
    "        \n",
    "    # Compute the weighted sum of the loss terms\n",
    "    loss_weights_normalized = F.softmax(loss_weights, dim=0)\n",
    "    total_loss = (loss_weights_normalized[0] * autoencoder_loss +\n",
    "                        loss_weights_normalized[1] * predictor_loss)\n",
    "                \n",
    "    # Backpropagation and optimization\n",
    "    autoencoder_optimizer.zero_grad()\n",
    "    predictor_optimizer.zero_grad()\n",
    "    optimizer_weights.zero_grad()\n",
    "        \n",
    "    total_loss.backward()\n",
    "\n",
    "    autoencoder_optimizer.step()\n",
    "    predictor_optimizer.step()\n",
    "    optimizer_weights.step()\n",
    "            \n",
    "    if ((epoch % 50) == 0):\n",
    "        print(f'Epoch [{epoch}/{num_epochs}], Total Loss: {total_loss.item():.3f}, Autoencoder Loss: {autoencoder_loss.item():.3f}, Predictor Loss: {predictor_loss:.3f}')\n",
    "        print (f'Autoencoder Weights: {loss_weights_normalized[0].item():.3f}, Predictor Weights: {loss_weights_normalized[1].item():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_new_label(all_predictions, x_unlabeled, confidence, input_size, y_unlabeled, known_celltypes, y_labeled, x_labeled, print = False, apply_confidence = False):\n",
    "\n",
    "    rows_to_remove = []\n",
    "    gt_pseudo_labeled = []\n",
    "    should_have_labeled = 0\n",
    "\n",
    "    pseudo_labeled_x, pseudo_labeled_y = initialize_pseudo_labeled(input_size)\n",
    "    \n",
    "    for index in range(x_unlabeled.shape[0]):\n",
    "        col_indices = np.where(all_predictions[index] == 1)[0]\n",
    "\n",
    "        if len(col_indices) == 0:\n",
    "            continue\n",
    "                \n",
    "        if len(col_indices) == 1:\n",
    "            pseudo_labeled_x, pseudo_labeled_y, rows_to_remove, gt_pseudo_labeled, should_have_labeled = \\\n",
    "                handle_single_class_sample(index, x_unlabeled, col_indices, pseudo_labeled_x, pseudo_labeled_y, rows_to_remove, gt_pseudo_labeled, y_unlabeled, known_celltypes, should_have_labeled)\n",
    "        elif len(col_indices) > 1:\n",
    "            pseudo_labeled_x, pseudo_labeled_y, rows_to_remove, gt_pseudo_labeled, should_have_labeled = \\\n",
    "                handle_multi_class_sample(index, x_unlabeled, col_indices, pseudo_labeled_x, pseudo_labeled_y, rows_to_remove, gt_pseudo_labeled, y_unlabeled, known_celltypes, y_labeled, x_labeled, confidence, should_have_labeled, apply_confidence)\n",
    "    \n",
    "    if (print):\n",
    "        print_results(rows_to_remove, sampled_indices, should_have_labeled)\n",
    "\n",
    "    return rows_to_remove, gt_pseudo_labeled, pseudo_labeled_x, pseudo_labeled_y\n",
    "\n",
    "def initialize_pseudo_labeled(input_size):\n",
    "    pseudo_labeled_x = torch.empty((0, input_size), dtype=torch.float32).to(device)\n",
    "    pseudo_labeled_y = torch.empty((0, 1), dtype=torch.long).to(device)\n",
    "    return pseudo_labeled_x, pseudo_labeled_y\n",
    "\n",
    "def handle_single_class_sample(index, x_unlabeled, col_indices, pseudo_labeled_x, pseudo_labeled_y, rows_to_remove, gt_pseudo_labeled, y_unlabeled, known_celltypes, should_have_labeled):\n",
    "    new_labeled_sample = np.expand_dims(x_unlabeled[index].cpu().detach().numpy(), axis=0)\n",
    "    new_label = col_indices.reshape(1, 1)\n",
    "\n",
    "    rows_to_remove.append(index)\n",
    "    pseudo_labeled_x = torch.cat((pseudo_labeled_x, torch.from_numpy(new_labeled_sample).to(device)), 0)\n",
    "    pseudo_labeled_y = torch.cat((pseudo_labeled_y, torch.from_numpy(new_label).to(device)), 0)\n",
    "    gt_pseudo_labeled.append(y_unlabeled[index])\n",
    "    \n",
    "    if y_unlabeled[index].cpu().detach().numpy() in np.array(known_celltypes):\n",
    "        should_have_labeled += 1\n",
    "    \n",
    "    return pseudo_labeled_x, pseudo_labeled_y, rows_to_remove, gt_pseudo_labeled, should_have_labeled\n",
    "\n",
    "def handle_multi_class_sample(index, x_unlabeled, col_indices, pseudo_labeled_x, pseudo_labeled_y, rows_to_remove, gt_pseudo_labeled, y_unlabeled, known_celltypes, y_labeled, x_labeled, confidence, should_have_labeled, apply_confidence):\n",
    "    selector = np.isin(y_labeled.cpu().detach().numpy(), col_indices)\n",
    "    tmp_x = x_labeled[selector].cpu().detach().numpy()\n",
    "    tmp_y = y_labeled[selector].cpu().detach().numpy()\n",
    "\n",
    "    n_neighbors = 20\n",
    "    nearNei = NearestNeighbors(n_neighbors=n_neighbors).fit(tmp_x)\n",
    "    distances, indices = nearNei.kneighbors(x_unlabeled[index].cpu().detach().numpy().reshape(1, -1))\n",
    "    nearest_labels = tmp_y[indices[0]]\n",
    "\n",
    "    label_counter = Counter(nearest_labels)\n",
    "    most_common_label, count = label_counter.most_common(1)[0]\n",
    "\n",
    "    if (apply_confidence):\n",
    "        if count > (n_neighbors / 2) + confidence:\n",
    "            new_labeled_sample = np.expand_dims(x_unlabeled[index].cpu().detach().numpy(), axis=0)\n",
    "            new_label = np.asarray(most_common_label).reshape(1, 1)\n",
    "\n",
    "            rows_to_remove.append(index)\n",
    "            pseudo_labeled_x = torch.cat((pseudo_labeled_x, torch.from_numpy(new_labeled_sample).to(device)), 0)\n",
    "            pseudo_labeled_y = torch.cat((pseudo_labeled_y, torch.from_numpy(new_label).to(device)), 0)\n",
    "            gt_pseudo_labeled.append(y_unlabeled[index])\n",
    "\n",
    "            if y_unlabeled[index].cpu().detach().numpy() in np.array(known_celltypes):\n",
    "                should_have_labeled += 1\n",
    "    else:\n",
    "        new_labeled_sample = np.expand_dims(x_unlabeled[index].cpu().detach().numpy(), axis=0)\n",
    "        new_label = np.asarray(most_common_label).reshape(1, 1)\n",
    "\n",
    "        rows_to_remove.append(index)\n",
    "        pseudo_labeled_x = torch.cat((pseudo_labeled_x, torch.from_numpy(new_labeled_sample).to(device)), 0)\n",
    "        pseudo_labeled_y = torch.cat((pseudo_labeled_y, torch.from_numpy(new_label).to(device)), 0)\n",
    "        gt_pseudo_labeled.append(y_unlabeled[index])\n",
    "\n",
    "        if y_unlabeled[index].cpu().detach().numpy() in np.array(known_celltypes):\n",
    "            should_have_labeled += 1\n",
    "    \n",
    "    return pseudo_labeled_x, pseudo_labeled_y, rows_to_remove, gt_pseudo_labeled, should_have_labeled\n",
    "\n",
    "def print_results(rows_to_remove, sampled_indices, should_have_labeled):\n",
    "    \n",
    "    total_labeled_samples = len(rows_to_remove)\n",
    "    print('Total labeled samples is %d' % total_labeled_samples)\n",
    "    print('Should have labeled %d from known classes' % len(sampled_indices))\n",
    "    print('Actually labeled from known classes %d' % should_have_labeled)\n",
    "    print('Known samples missing %d' % (len(sampled_indices) - should_have_labeled))\n",
    "    print('Wrongly labeled %d' % (total_labeled_samples - should_have_labeled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_class_svm(train_encoded, test_encoded, nu, x_unlabeled, y_labeled, known_celltypes, confidence = 5, print = False, apply_confidence = False):\n",
    "\n",
    "    all_predictions = []\n",
    "\n",
    "    #train one-class SVM for each known cell type and perform testing on sampled unknown cells\n",
    "    for k_class in np.unique(known_celltypes):\n",
    "                \n",
    "        x_train = train_encoded[y_labeled == k_class]\n",
    "                \n",
    "        ## OC-SVM MODEL\n",
    "        SVM_model = OneClassSVM(kernel='rbf', gamma='auto', nu=nu, shrinking=True)\n",
    "        SVM_model.fit(x_train.cpu().detach().numpy())\n",
    "        predictions = SVM_model.predict(test_encoded.cpu().detach().numpy())\n",
    "        all_predictions.append(predictions)\n",
    "\n",
    "    all_predictions = np.array(all_predictions).squeeze()\n",
    "    all_predictions = all_predictions.T\n",
    "\n",
    "    #Find new labels\n",
    "    rows_to_remove, gt_pseudo_labeled, pseudo_labeled_x, pseudo_labeled_y = find_new_label(all_predictions, x_unlabeled, confidence, input_size, y_unlabeled, known_celltypes, y_labeled, x_labeled, print, apply_confidence)\n",
    "    return rows_to_remove, gt_pseudo_labeled, pseudo_labeled_x, pseudo_labeled_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence = 5\n",
    "nus = [0.0001, 0.001, 0.01, 0.1, 0.3]\n",
    "best_acc = 0\n",
    "\n",
    "for nu in nus:\n",
    "    print (\"Running with nu = \", nu)\n",
    "    rows_to_remove, gt_pseudo_labeled, pseudo_labeled_x, pseudo_labeled_y = one_class_svm(autoencoder(x_labeled)[0], autoencoder(x_unlabeled)[0], nu, x_unlabeled, y_labeled, known_celltypes, \n",
    "                                                                                          confidence, print = True, apply_confidence = False)\n",
    "    \n",
    "    correct = (torch.squeeze(pseudo_labeled_y) == torch.tensor(gt_pseudo_labeled)).float().sum()\n",
    "    known_acc = correct / pseudo_labeled_y.shape[0]\n",
    "    print(f'Supervised Accuracy: {known_acc.item()}')\n",
    "\n",
    "    if (best_acc < known_acc.item()):\n",
    "        best_acc = known_acc.item()\n",
    "        best_nu = nu\n",
    "\n",
    "    print ('-----------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_nu = 0.0001\n",
    "rows_to_remove, gt_pseudo_labeled, pseudo_labeled_x, pseudo_labeled_y = one_class_svm(autoencoder(x_labeled)[0], autoencoder(x_unlabeled)[0], best_nu, x_unlabeled, y_labeled, known_celltypes, \n",
    "                                                                                      confidence, print = True, apply_confidence = False)\n",
    "correct = (torch.squeeze(pseudo_labeled_y) == torch.tensor(gt_pseudo_labeled)).float().sum()\n",
    "known_acc = correct / pseudo_labeled_y.shape[0]\n",
    "print(f'Supervised Accuracy: {known_acc.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the rows from the unknown data\n",
    "mask = torch.ones(x_unlabeled.size(0), dtype=torch.bool)\n",
    "mask[rows_to_remove] = 0\n",
    "x_unlabeled = x_unlabeled[mask]\n",
    "y_unlabeled = y_unlabeled[mask]\n",
    "unknown_cell_names = np.delete(unknown_cell_names, rows_to_remove, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = torch.squeeze(pseudo_labeled_y).cpu().detach().numpy()\n",
    "tmp2 = torch.tensor(gt_pseudo_labeled, device = 'cpu').numpy()\n",
    "\n",
    "subselected_pseudo_y = []\n",
    "subselected_gt = []\n",
    "\n",
    "for i in range(len(tmp)):\n",
    "    if (tmp[i] in known_celltypes and tmp2[i] in known_celltypes):\n",
    "        subselected_pseudo_y.append(tmp[i])\n",
    "        subselected_gt.append(tmp2[i])\n",
    "\n",
    "con = confusion_matrix(subselected_gt, subselected_pseudo_y)\n",
    "cm_normalized = con.astype('float') / con.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.1f', cmap='Blues', xticklabels=known_mapping.keys(), \n",
    "             yticklabels=known_mapping.keys())\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_tensor(tensor, mapping):\n",
    "    # Vectorize the dictionary lookup function\n",
    "    vectorized_map = np.vectorize(lambda x: mapping.get(x, x))\n",
    "    return vectorized_map(tensor)\n",
    "\n",
    "all_labeled_x = torch.cat((x_labeled, pseudo_labeled_x))\n",
    "all_labeled_y = torch.cat((y_labeled, torch.squeeze(pseudo_labeled_y)))\n",
    "\n",
    "# Map the integer labels back to their original string values\n",
    "string_labels = map_tensor(all_labeled_y, inv_known_mapping)\n",
    "\n",
    "# Rember which ones are labeled and which ones are pseudo-labeled\n",
    "labeled_pseudo = np.concatenate([np.zeros(x_labeled.shape[0]), np.ones(pseudo_labeled_x.shape[0])])\n",
    "\n",
    "# Look at the wrongly pseudo labeled\n",
    "comparison_array = torch.where(torch.squeeze(pseudo_labeled_y) == torch.tensor(gt_pseudo_labeled), 1, 2)\n",
    "comparison_array_np = comparison_array.numpy()\n",
    "wrongly_labeled = np.concatenate([np.zeros(x_labeled.shape[0]), comparison_array_np])\n",
    "\n",
    "# Look at the samples that should have not been pseudo-labeled\n",
    "should_have_not_been_labeled = np.array([1 if elem in known_celltypes else 2 for elem in torch.tensor(gt_pseudo_labeled).tolist()])\n",
    "should_have_not_been_labeled = np.concatenate([np.zeros(x_labeled.shape[0]), should_have_not_been_labeled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder= autoencoder.to(device)\n",
    "encoded_data = autoencoder(all_labeled_x)[0].cpu().detach().numpy()\n",
    "mapper = umap.UMAP(n_neighbors=15, min_dist=0.3)\n",
    "embedding = mapper.fit_transform(encoded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_umap = mapper.fit_transform(all_labeled_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 2, figsize=(20,30))\n",
    "ax[0,0].scatter(embedding[:, 0], embedding[:, 1], s=0.5, c=[(0.8, 0.8, 0.8) for x in string_labels])\n",
    "ax[0,1].scatter(embedding[:, 0], embedding[:, 1], s=0.5, c=[color_map[x] for x in string_labels])\n",
    "ax[0,1].set_xlabel('UMAP 1')\n",
    "ax[0,1].set_ylabel('UMAP 2')\n",
    "ax[0,1].set_yticklabels([])\n",
    "ax[0,1].set_xticklabels([])\n",
    "colors = [color_map[celltype] if pseudo == 1 else (0.8, 0.8, 0.8)for celltype, pseudo in zip(string_labels, labeled_pseudo)]\n",
    "ax[1,0].scatter(embedding[:, 0], embedding[:, 1], s=1, c=colors)\n",
    "colors = [color_map[celltype] if pseudo == 0 else (0.8, 0.8, 0.8)for celltype, pseudo in zip(string_labels, labeled_pseudo)]\n",
    "ax[1,1].scatter(embedding[:, 0], embedding[:, 1], s=1, c=colors)\n",
    "colors = [(0.8, 0.8, 0.8) if labeled == 0 else (0, 1, 0) if labeled == 1 else (1, 0, 0) for labeled in wrongly_labeled]\n",
    "ax[2,0].scatter(embedding[:, 0], embedding[:, 1], s=1, c=colors)\n",
    "colors = [(0.8, 0.8, 0.8) if labeled == 0 else (0, 1, 0) if labeled == 1 else (1, 0, 0) for labeled in should_have_not_been_labeled]\n",
    "ax[2,1].scatter(embedding[:, 0], embedding[:, 1], s=1, c=colors)\n",
    "#ax[2,2].scatter(pca_umap[:, 0], pca_umap[:, 1], s=0.5, c=[color_map[x] for x in string_labels])\n",
    "\n",
    "\n",
    "df = pd.DataFrame(dict(ax0=embedding[:, 0], ax1=embedding[:, 1], celltype=string_labels))\n",
    "mean_values = df.groupby('celltype').mean()\n",
    "# mean_values[\"ax1\"].iloc[2] = mean_values[\"ax1\"].iloc[2] - 1\n",
    "# mean_values[\"ax1\"].iloc[1] = mean_values[\"ax1\"].iloc[1] + 0.5\n",
    "# mean_values[\"ax0\"].iloc[9] = mean_values[\"ax0\"].iloc[9] - 1\n",
    "# mean_values[\"ax1\"].iloc[9] = mean_values[\"ax1\"].iloc[9] + 1\n",
    "# mean_values[\"ax1\"].iloc[3] = mean_values[\"ax1\"].iloc[3] + 1.5\n",
    "# mean_values[\"ax0\"].iloc[4] = mean_values[\"ax0\"].iloc[4] + 1\n",
    "# mean_values[\"ax1\"].iloc[7] = mean_values[\"ax1\"].iloc[7] - 1\n",
    "\n",
    "for name, row in mean_values.iterrows():\n",
    "    ax[0,1].text(row.ax0, row.ax1, name, va=\"center\", ha=\"center\", fontdict={\"weight\": \"bold\"},fontsize=12)\n",
    "    \n",
    "ax[0,0].set_title('Labeled and pseudo labeled')\n",
    "ax[0,1].set_title('Labeled and pseudo labeled')\n",
    "ax[1,0].set_title('Labeled and pseudo labeled - Pseudo labeled')\n",
    "ax[1,1].set_title('Labeled and pseudo labeled - Labeled')\n",
    "ax[2,0].set_title('Wrongly pseudo labeled')\n",
    "ax[2,1].set_title('Should have not been pseudo labeled')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(np.sqrt(len(np.unique(string_labels)))) + 1\n",
    "\n",
    "fig, ax = plt.subplots(n, n, figsize=(40,40))\n",
    "\n",
    "for name, axs in zip(np.unique(string_labels), ax.ravel()):\n",
    "    \n",
    "    row = mean_values[mean_values.index == name]\n",
    "\n",
    "    axs.scatter(embedding[:, 0], embedding[:, 1], s=1, c=[color_map[x] if x == name else (0.8, 0.8, 0.8) for x in string_labels])\n",
    "    axs.text(row.ax0.to_numpy(), row.ax1.to_numpy(), name, va=\"center\", ha=\"center\", fontdict={\"weight\": \"bold\"})\n",
    "    axs.axis(\"off\")\n",
    "    axs.set_title(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Number of unlabeled samples', x_unlabeled.shape[0])\n",
    "print ('Unkown classes: %d' % len(unknown_celltypes))\n",
    "np.unique(y_unlabeled.cpu().detach().numpy(), return_counts=True)\n",
    "assert (len(unknown_cell_names) == x_unlabeled.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_adjacency_matrix(data, num_neighbors = 50):\n",
    "\n",
    "    num_samples = data.shape[0]\n",
    "    print ('Computing adjacency using %d neighbors' %num_neighbors)\n",
    "    # Number of neighbors to retrieve\n",
    "    num_neighbors = num_neighbors\n",
    "\n",
    "    # Create a cKDTree from the features matrix\n",
    "    kdtree = cKDTree(data)\n",
    "\n",
    "    # List to store nearest neighbors for each point\n",
    "    nearest_neighbors_list = []\n",
    "\n",
    "    # Query for the nearest neighbors for each point in features_matrix\n",
    "    for i in range(num_samples):\n",
    "        query_point = data[i]\n",
    "        distances, indices = kdtree.query(query_point, k=num_neighbors)\n",
    "        nearest_neighbors_list.append(indices)\n",
    "\n",
    "    adjacency_matrix = np.zeros((num_samples, num_samples))\n",
    "\n",
    "    # Populate the adjacency matrix based on nearest neighbors list\n",
    "    for i in range(num_samples):\n",
    "        nearest_neighbors_indices = nearest_neighbors_list[i]\n",
    "        adjacency_matrix[i, nearest_neighbors_indices] = 1\n",
    "\n",
    "    return adjacency_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, n_hidden, n_layers, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        # input layer\n",
    "        self.layers.append(GCNConv(in_feats, n_hidden))\n",
    "\n",
    "        # hidden layers\n",
    "        for _ in range(n_layers - 1):\n",
    "            self.layers.append(GCNConv(n_hidden, n_hidden))\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if i != 0:\n",
    "                x = self.dropout(x)\n",
    "            x = F.relu(layer(x, edge_index))\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_feats, n_hidden, n_layers, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv = GCN(in_feats, n_hidden, n_layers, dropout)\n",
    "\n",
    "    def forward(self, data, corrupt=False):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        if corrupt:\n",
    "            perm = torch.randperm(x.size(0))\n",
    "            x = x[perm]\n",
    "        x = self.conv(x, edge_index)\n",
    "        return x\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, n_hidden):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.Tensor(n_hidden, n_hidden))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def uniform(self, size, tensor):\n",
    "        bound = 1.0 / torch.sqrt(torch.tensor(size, dtype=torch.float))\n",
    "        if tensor is not None:\n",
    "            tensor.data.uniform_(-bound, bound)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        size = self.weight.size(0)\n",
    "        self.uniform(size, self.weight)\n",
    "\n",
    "    def forward(self, features, summary):\n",
    "        features = torch.matmul(features, torch.matmul(self.weight, summary))\n",
    "        return features\n",
    "\n",
    "class DGI(nn.Module):\n",
    "    def __init__(self, in_feats, n_hidden, n_layers, dropout):\n",
    "        super(DGI, self).__init__()\n",
    "        self.encoder = Encoder(in_feats, n_hidden, n_layers, dropout)\n",
    "        self.discriminator = Discriminator(n_hidden)\n",
    "        self.loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, data):\n",
    "        positive = self.encoder(data, corrupt=False)\n",
    "        negative = self.encoder(data, corrupt=True)\n",
    "        summary = torch.sigmoid(positive.mean(dim=0))\n",
    "\n",
    "        positive = self.discriminator(positive, summary)\n",
    "        negative = self.discriminator(negative, summary)\n",
    "\n",
    "        l1 = self.loss(positive, torch.ones_like(positive))\n",
    "        l2 = self.loss(negative, torch.zeros_like(negative))\n",
    "\n",
    "        return l1 + l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useGraph = True\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print ('Using ', device)\n",
    "\n",
    "if (useGraph):\n",
    "    print ('Embedding unknown cell before clustering with GNNs...')\n",
    "    adj_matrix = create_adjacency_matrix(x_unlabeled)\n",
    "    adj_matrix = torch.tensor(adj_matrix, dtype=torch.long)\n",
    "    edge_index = adj_matrix.nonzero().t().contiguous()\n",
    "\n",
    "    x_feat = x_unlabeled\n",
    "    data = Data(x=x_feat, edge_index=edge_index)\n",
    "    data = data.to(device)\n",
    "\n",
    "    in_feats = x_unlabeled.shape[1]\n",
    "    n_hidden = 32\n",
    "    n_layers = 2\n",
    "    dropout = 0.5\n",
    "    num_epochs = 500\n",
    "\n",
    "    model = DGI(in_feats, n_hidden, n_layers, dropout).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if ((epoch % 100) == 0):\n",
    "            print(f'Epoch [{epoch}/{num_epochs}], Loss: {total_loss:.4f}')\n",
    "\n",
    "    # After training, get node embeddings\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        node_embeddings = model.encoder(data, corrupt=False)\n",
    "\n",
    "else: \n",
    "    node_embeddings = x_unlabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_acc(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Calculate clustering accuracy.\n",
    "    # Arguments\n",
    "        y: true labels, numpy.array with shape `(n_samples,)`\n",
    "        y_pred: predicted labels, numpy.array with shape `(n_samples,)`\n",
    "    # Return\n",
    "        accuracy, in [0,1]\n",
    "    \"\"\"\n",
    "    y_true = y_true.astype(np.int64)\n",
    "    assert y_pred.size == y_true.size\n",
    "    D = max(y_pred.max(), y_true.max()) + 1\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    for i in range(y_pred.size):\n",
    "        w[y_pred[i], y_true[i]] += 1\n",
    "    row_ind, col_ind = linear_sum_assignment(w.max() - w)\n",
    "\n",
    "    return w[row_ind, col_ind].sum() / y_pred.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_upper_bound = 14\n",
    "node_embeddings = node_embeddings.detach().numpy()\n",
    "print ('Running unsupervised clustering with %d components' %component_upper_bound)\n",
    "DPGMM = mixture.BayesianGaussianMixture(n_components=component_upper_bound, \n",
    "                                                max_iter=50,\n",
    "                                                n_init=20,\n",
    "                                                tol=1e-5, \n",
    "                                                init_params='k-means++', \n",
    "                                                weight_concentration_prior_type='dirichlet_process', verbose=0)\n",
    "DPGMM.fit(node_embeddings)\n",
    "print ('Unsupervised clustering done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = DPGMM.weights_\n",
    "threshold = 0.05\n",
    "\n",
    "# Count the number of significant components (weights above threshold)\n",
    "significant_components = np.sum(weights >= threshold)\n",
    "print ('Significant components: %d' %significant_components)\n",
    "print ('Number of true classes: %d' %len(unknown_celltypes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DPGMM = mixture.BayesianGaussianMixture(n_components=significant_components, covariance_type = 'full',\n",
    "                                                max_iter=50,\n",
    "                                                n_init=50,\n",
    "                                                tol=1e-5, \n",
    "                                                init_params='k-means++', \n",
    "                                                weight_concentration_prior_type='dirichlet_process', verbose=0)\n",
    "DPGMM.fit(node_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusts = pd.DataFrame(DPGMM.predict(node_embeddings))\n",
    "cluster_probabilities = DPGMM.predict_proba(node_embeddings)\n",
    "\n",
    "unknown_acc = cluster_acc(np.squeeze(clusts.to_numpy()), y_unlabeled.numpy())\n",
    "print ('Unupervised Accuracy', unknown_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = clusts + len(known_celltypes)\n",
    "all_labeled_x = torch.cat((pseudo_labeled_x, x_unlabeled))\n",
    "all_labeled_y = torch.cat((torch.squeeze(pseudo_labeled_y), torch.tensor(np.squeeze(tmp.to_numpy()))))\n",
    "\n",
    "# +1 because the sample itself is included\n",
    "neighbors = 10\n",
    "nnn = NearestNeighbors(n_neighbors=neighbors + 1)  \n",
    "nnn.fit(all_labeled_x)\n",
    "distances, indices = nnn.kneighbors(all_labeled_x)\n",
    "\n",
    "# Determine the most common cluster among the neighbors\n",
    "# Exclude the first neighbor because it is the sample itself\n",
    "\n",
    "neighbor_clusters = all_labeled_y[indices[:, 1:]]  # This slices off the first column\n",
    "new_clusters = np.array([mode(neighbor_clusters[i])[0][0] for i in range(neighbor_clusters.shape[0])])\n",
    "print ('Superised & Unupervised Accuracy', cluster_acc(all_labeled_y.numpy(), new_clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsupervised_clusters = tmp[0].to_numpy()\n",
    "mapper = umap.UMAP(n_neighbors=15, min_dist=0.3)\n",
    "embedding = mapper.fit_transform(node_embeddings)\n",
    "\n",
    "# Look at the samples that should have been pseudo-labeled\n",
    "should_have_been_labeled =  np.array([1 if elem in known_celltypes else 0 for elem in y_unlabeled.numpy()])\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(20,20))\n",
    "ax[0,0].scatter(embedding[:, 0], embedding[:, 1], s=1, c=[(0.8, 0.8, 0.8) for x in unsupervised_clusters])\n",
    "ax[0,1].scatter(embedding[:, 0], embedding[:, 1], s=1, c=[color_map[x] for x in unsupervised_clusters])\n",
    "ax[1,0].scatter(embedding[:, 0], embedding[:, 1], s=1, c=[color_map[x] for x in map_tensor(y_unlabeled, inv_all_mapping)])\n",
    "colors = [(1, 0, 0) if labeled == 1 else (0, 1, 0) for labeled in should_have_been_labeled]\n",
    "ax[1,1].scatter(embedding[:, 0], embedding[:, 1], s=1, c=colors)\n",
    "\n",
    "df = pd.DataFrame(dict(ax0=embedding[:, 0], ax1=embedding[:, 1], celltype=unsupervised_clusters))\n",
    "mean_values = df.groupby('celltype').mean()\n",
    "\n",
    "for name, row in mean_values.iterrows():\n",
    "    ax[0,1].text(row.ax0, row.ax1, name, va=\"center\", ha=\"center\", fontdict={\"weight\": \"bold\"},fontsize=12)\n",
    "\n",
    "df = pd.DataFrame(dict(ax0=embedding[:, 0], ax1=embedding[:, 1], celltype=map_tensor(y_unlabeled, inv_all_mapping)))\n",
    "mean_values = df.groupby('celltype').mean()\n",
    "\n",
    "for name, row in mean_values.iterrows():\n",
    "    ax[1,0].text(row.ax0, row.ax1, name, va=\"center\", ha=\"center\", fontdict={\"weight\": \"bold\"},fontsize=10)\n",
    "\n",
    "ax[0,0].set_title('Unlabeled clustering')\n",
    "ax[0,1].set_title('Unlabeled clustering')\n",
    "ax[1,0].set_title('Unlabeled with ground truth')\n",
    "ax[1,1].set_title('Should have pseudo labeled')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differential Expression on unlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Reading normalized unlabeled data for differential expression\")\n",
    "files = [f for f in listdir(join(data_dir, dataset)) if isfile(join(data_dir, dataset, f))]\n",
    "\n",
    "for file in files:\n",
    "    if ('EXPR' in file):\n",
    "        exp = pd.read_csv(join(data_dir, dataset, file))\n",
    "        exp = exp.T\n",
    "        exp.columns = exp.iloc[0]\n",
    "        exp = exp.drop(exp.index[0])\n",
    "        exp = exp.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "exp = exp.loc[unknown_cell_names]\n",
    "unsupervised_clusters = unsupervised_clusters.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an AnnData object\n",
    "adata = AnnData(X=exp.values, obs={'cluster': unsupervised_clusters}, var=pd.DataFrame(index=exp.columns))\n",
    "adata.obs['cluster'] = adata.obs['cluster'].astype(str).astype('category')\n",
    "print(adata)\n",
    "sc.tl.rank_genes_groups(adata, 'cluster', method='wilcoxon', n_genes=20)\n",
    "sc.pl.rank_genes_groups_heatmap(adata, n_genes=5, groupby=\"cluster\", show_gene_labels=True)\n",
    "sc.pl.rank_genes_groups_dotplot(adata, n_genes=5, groupby=\"cluster\")\n",
    "sc.pl.rank_genes_groups_matrixplot(adata, n_genes=5, groupby=\"cluster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = adata.uns['rank_genes_groups']\n",
    "groups = result['names'].dtype.names\n",
    "\n",
    "# Extract the differentially expressed genes and their metrics\n",
    "de_genes = pd.DataFrame(\n",
    "    {group: result['names'][group] for group in groups}\n",
    ")\n",
    "scores = pd.DataFrame(\n",
    "    {group: result['scores'][group] for group in groups}\n",
    ")\n",
    "logfoldchanges = pd.DataFrame(\n",
    "    {group: result['logfoldchanges'][group] for group in groups}\n",
    ")\n",
    "pvals = pd.DataFrame(\n",
    "    {group: result['pvals'][group] for group in groups}\n",
    ")\n",
    "pvals_adj = pd.DataFrame(\n",
    "    {group: result['pvals_adj'][group] for group in groups}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import decoupler as dc\n",
    "# Query Omnipath and get PanglaoDB\n",
    "markers = dc.get_resource('PanglaoDB')\n",
    "markers = markers[markers['human'] & markers['canonical_marker'] & (markers['human_sensitivity'] > 0.5)]\n",
    "# Remove duplicated entries\n",
    "markers = markers[~markers.duplicated(['cell_type', 'genesymbol'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc.run_ora(\n",
    "    mat=adata,\n",
    "    net=markers,\n",
    "    source='cell_type',\n",
    "    target='genesymbol',\n",
    "    min_n=3,\n",
    "    verbose=True,\n",
    "    use_raw=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acts = dc.get_acts(adata, obsm_key='ora_estimate')\n",
    "# We need to remove inf and set them to the maximum value observed for pvals=0\n",
    "acts_v = acts.X.ravel()\n",
    "max_e = np.nanmax(acts_v[np.isfinite(acts_v)])\n",
    "acts.X[~np.isfinite(acts.X)] = max_e\n",
    "df = dc.rank_sources_groups(acts, groupby='cluster', reference='rest', method='t-test_overestim_var')\n",
    "n_ctypes = 3\n",
    "ctypes_dict = df.groupby('group').head(n_ctypes).groupby('group')['names'].apply(lambda x: list(x)).to_dict()\n",
    "ctypes_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = adata.uns['rank_genes_groups']\n",
    "groups = result['names'].dtype.names\n",
    "\n",
    "# Extract the differentially expressed genes\n",
    "de_genes = pd.DataFrame(\n",
    "    {group: result['names'][group] for group in groups}\n",
    ")\n",
    "\n",
    "# Print the top 20 genes for each cluster in the desired format\n",
    "for group in groups:\n",
    "    top_genes = de_genes[group].head(20).tolist()  # Get the top 20 genes\n",
    "    genes_str = \", \".join(top_genes)\n",
    "    print(f\"{group}: {genes_str}\")\n",
    "\n",
    "print ('Unknow types:', np.array(unknown_celltypes_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multiomics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
